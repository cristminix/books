<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style00.css" rel="stylesheet" type="text/css" />

<style type="text/css">body{margin:1em;background-color:transparent!important;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content"><div class="readable-text" id="p1"> &#13;
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">10</span></span> <span class="chapter-title-text">Agent reasoning and evaluation</span></h1> &#13;
  </div> &#13;
  <div class="introduction-summary"> &#13;
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> &#13;
   <ul> &#13;
    <li class="readable-text" id="p2">Using various prompt engineering techniques to extend large language model functions</li> &#13;
    <li class="readable-text" id="p3">Engaging large language models with prompt engineering techniques that engage reasoning</li> &#13;
    <li class="readable-text" id="p4">Employing an evaluation prompt to narrow and identify the solution to an unknown problem </li> &#13;
   </ul> &#13;
  </div> &#13;
  <div class="readable-text" id="p5"> &#13;
   <p>Now that we’ve examined the patterns of memory and retrieval that define the semantic memory component in agents, we can take a look at the last and most instrumental component in agents: planning. Planning encompasses many facets, from reasoning, understanding, and evaluation to feedback.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p6"> &#13;
   <p>To explore how LLMs can be prompted to reason, understand, and plan, we’ll demonstrate how to engage reasoning through prompt engineering and then expand that to planning. The planning solution provided by the Semantic Kernel (SK) encompasses multiple planning forms. We’ll finish the chapter by incorporating adaptive feedback into a new planner.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p7"> &#13;
   <p>Figure 10.1 demonstrates the high-level prompt engineering strategies we’ll cover in this chapter and how they relate to the various techniques we’ll cover. Each of the methods showcased in the figure will be explored in this chapter, from the basics of solution/direct prompting, shown in the top-left corner, to self-consistency and tree of thought (ToT) prompting, in the bottom right.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p8">  &#13;
   <img alt="figure" src="Images/10-1.png" width="1009" height="914"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.1</span> How the two planning prompt engineering strategies align with the various techniques</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p9"> &#13;
   <h2 class="readable-text-h2" id="sigil_toc_id_136"><span class="num-string">10.1</span> Understanding direct solution prompting</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p10"> &#13;
   <p><em>Direct solution prompting</em> is generally the first form of prompt engineering that users employ when asking LLMs questions or solving a particular problem. Given any LLM use, these techniques may seem apparent, but they are worth reviewing to establish the foundation of thought and planning. In the next section, we’ll start from the beginning, asking questions and expecting answers.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p11"> &#13;
   <h3 class="readable-text-h3" id="sigil_toc_id_137"><span class="num-string">10.1.1</span> Question-and-answer prompting</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p12"> &#13;
   <p>For the exercises in this chapter, we’ll employ prompt flow to build and evaluate the various techniques. (We already extensively covered this tool in chapter 9, so refer to that chapter if you need a review.) Prompt flow is an excellent tool for understanding how these techniques work and exploring the flow of the planning and reasoning process.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p13"> &#13;
   <p>Open Visual Studio Code (VS Code) to the <code>chapter</code> <code>10</code> source folder. Create a new virtual environment for the folder, and install the <code>requirements.txt</code> file. If you need help setting up a chapter’s Python environment, refer to appendix B.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p14"> &#13;
   <p>We’ll look at the first flow in the <code>prompt_flow/question-answering-prompting</code> folder. Open the <code>flow.dag.yaml</code> file in the visual editor, as shown in figure 10.2. On the right side, you’ll see the flow of components. At the top is the <code>question_answer</code> LLM prompt, followed by two <code>Embedding</code> components and a final LLM prompt to do the evaluation called <code>evaluate</code>.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p15">  &#13;
   <img alt="figure" src="Images/10-2.png" width="1012" height="524"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.2</span> The <code>flow.dag.yaml</code> file, open in the visual editor, highlighting the various components of the flow</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p16"> &#13;
   <p>The breakdown in listing 10.1 shows the structure and components of the flow in more detail using a sort of YAML-shortened pseudocode. You can also see the input and outputs to the various components and a sample output from running the flow.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p17"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.1</span> <code>question-answer-prompting</code> flow</h5> &#13;
   <div class="code-area-container code-area-with-html"> &#13;
    <pre class="code-area">   Inputs:&#13;
        context  : the content to ask the question about&#13;
        question : question asked specific to the content&#13;
        expected : the expected answer&#13;
&#13;
<strong>   </strong>LLM: Question-Answer (the prompt used to ask the question)&#13;
        inputs:&#13;
               context and question&#13;
        outputs: &#13;
               the prediction/answer to the question&#13;
&#13;
   Embeddings: uses an LLM embedding model to create the embedding &#13;
representation of the text&#13;
&#13;
     Embedding_predicted: embeds the output of the Question-Answer LLM&#13;
     Embedding_expected: embeds the output of the expected answer&#13;
&#13;
<strong>   </strong>Python: Evaluation (Python code to measure embedding similarity)&#13;
     Inputs:&#13;
            Embedding_predicted output&#13;
            Embedding_expected output&#13;
     Outputs: &#13;
            the similarity score between predicted and expected&#13;
&#13;
   Outputs:&#13;
        context: -&gt; input.context&#13;
        question: -&gt; input.question&#13;
     expected: -&gt; input.expected&#13;
     predicted: -&gt; output.question_answer&#13;
     evaluation_score: output.evaluation&#13;
&#13;
### Example Output&#13;
{&#13;
    "context": "Back to the Future (1985)…",&#13;
    "evaluation_score": 0.9567478002354606,&#13;
    "expected": "Marty traveled back in time 30 years.",&#13;
    "predicted": "Marty traveled back in time 30 years from 1985 to 1955 &#13;
in the movie \"Back to the Future.\"",&#13;
    "question": "How far did Marty travel back in time in the movie &#13;
Back to the Future (1985)"&#13;
}</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p18"> &#13;
   <p>Before running this flow, make sure your LLM block is configured correctly. This may require you to set up a connection to your chosen LLM. Again, refer to chapter 9 if you need a review on how to complete this. You’ll need to configure the LLM and <code>Embedding</code> blocks with your connection if you’re not using OpenAI.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p19"> &#13;
   <p>After configuring your LLM connection, run the flow by clicking the Play button from the visual editor or using the Test (Shift-F5) link in the YAML editor window. If everything is connected and configured correctly, you should see output like that in listing 10.1.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p20"> &#13;
   <p>Open the <code>question_answer.jinja2</code> file in VS Code, as shown in listing 10.2. This listing shows the basic question-and-answer-style prompt. In this style of prompt, the system message describes the basic rules and provides the context to answer the question. In chapter 4, we explored the retrieval augmented generation (RAG) pattern, and this prompt follows a similar pattern.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p21"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.2</span> <code>question_answer.jinja2</code></h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">system:&#13;
Answer the users question based on the context below. Keep the answer &#13;
short and concise. Respond "Unsure about answer" if not sure about the &#13;
answer.&#13;
&#13;
Context: {{context}}    <span class="aframe-location"/> #1&#13;
&#13;
user:&#13;
Question: {{question}}    <span class="aframe-location"/> #2</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Replace with the content LLM should answer the question about.&#13;
     <br/>#2 Replace with the question.&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p22"> &#13;
   <p>This exercise shows the simple method of using an LLM to ask questions about a piece of content. Then, the question response is evaluated using a similarity matching score. We can see from the output in listing 10.1 that the LLM does a good job of answering a question about the context. In the next section, we’ll explore a similar technique that uses direct prompting.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p23"> &#13;
   <h3 class="readable-text-h3" id="sigil_toc_id_138"><span class="num-string">10.1.2</span> Implementing few-shot prompting</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p24"> &#13;
   <p><em>Few-shot prompting</em> is like question-and-answer prompting, but the makeup of the prompt is more about providing a few examples than about facts or context. This allows the LLM to bend to patterns or content not previously seen. While this approach sounds like question and answer, the implementation is quite different, and the results can be powerful.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p25"> &#13;
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Zero-shot, one-shot, and few-shot learning</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p26"> &#13;
    <p>One holy grail of machine learning and AI is the ability to train a model on as few items as possible. For example, in traditional vision models, millions of images are fed into the model to help identify the differences between a cat and a dog.</p> &#13;
   </div> &#13;
   <div class="readable-text" id="p27"> &#13;
    <p>A <em>one-shot</em> model is a model that requires only a single image to train it. For example, a picture of a cat can be shown, and then the model can identify any cat image. A <em>few-shot</em> model requires only a few things to train the model. And, of course, <em>zero-shot</em> indicates the ability to identify something given no previous examples. LLMs are efficient learners and can do all three types of learning.</p> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p28"> &#13;
   <p>Open <code>prompt_flow/few-shot-prompting/flow.dag.yaml</code> in VS Code and the visual editor. Most of the flow looks like the one pictured earlier in figure 10.2, and the differences are highlighted in listing 10.3, which shows a YAML pseudocode representation. The main differences between this and the previous flow are the inputs and LLM prompt.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p29"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.3</span> <code>few-shot-prompting</code> flow</h5> &#13;
   <div class="code-area-container code-area-with-html"> &#13;
    <pre class="code-area"><strong>   </strong>Inputs:&#13;
       statement  : introduces the context and then asks for output&#13;
       expected : the expected answer to the statement&#13;
<strong>  </strong> LLM: few_shot (the prompt used to ask the question)&#13;
       inputs:statement&#13;
       outputs: the prediction/answer to the statement&#13;
&#13;
<strong>   </strong>Embeddings: uses an LLM embedding model to create the embedding &#13;
representation of the text&#13;
&#13;
        Embedding_predicted: embeds the output of the few_shot LLM&#13;
        Embedding_expected: embeds the output of the expected answer&#13;
&#13;
   Python: Evaluation (Python code to measure embedding similarity)&#13;
        Inputs:&#13;
               Embedding_predicted output&#13;
               Embedding_expected output&#13;
        Outputs: the similarity score between predicted and expected&#13;
&#13;
Outputs:&#13;
        statement: -&gt; input.statement&#13;
        expected: -&gt; input.expected&#13;
        predicted: -&gt; output.few_shot&#13;
        evaluation_score: output.evaluation&#13;
&#13;
### Example Output&#13;
{&#13;
    "evaluation_score": 0.906647282920417,    <span class="aframe-location"/> #1&#13;
    "expected": "We ate sunner and watched the setting sun.",&#13;
    "predicted": "After a long hike, we sat by the lake &#13;
and enjoyed a peaceful sunner as the sky turned &#13;
brilliant shades of orange and pink.",    <span class="aframe-location"/> #2&#13;
    "statement": "A sunner is a meal we eat in Cananda &#13;
at sunset, please use the word in a sentence"    <span class="aframe-location"/> #3&#13;
}</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Evaluation score represents the similarity between expected and predicted.&#13;
     <br/>#2 Uses sunner in a sentence&#13;
     <br/>#3 This is a false statement but the intent is to get the LLM to use the word as if it was real.&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p30"> &#13;
   <p>Run the flow by pressing Shift-F5 or clicking the Play/Test button from the visual editor. You should see output like listing 10.3 where the LLM has used the word <em>sunner</em> (a made-up term) correctly in a sentence given the initial statement.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p31"> &#13;
   <p>This exercise demonstrates the ability to use a prompt to alter the behavior of the LLM to be contrary to what it has learned. We’re changing what the LLM understands to be accurate. Furthermore, we then use that modified perspective to elicit the use of a made-up word.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p32"> &#13;
   <p>Open the <code>few_shot.jinja2</code> prompt in VS Code, shown in listing 10.4. This listing demonstrates setting up a simple persona, that of an eccentric dictionary maker, and then providing examples of words it has defined and used before. The base of the prompt allows for the LLM to extend the examples and produce similar results using other words.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p33"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.4</span> <code>few_shot.jinja2</code></h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">system:&#13;
You are an eccentric word dictionary maker. You will be asked to &#13;
&#13;
construct a sentence using the word.&#13;
The following are examples that demonstrate how to craft a sentence using &#13;
the word.&#13;
A "whatpu" is a small, furry animal native to Tanzania. &#13;
An example of a sentence that uses the word whatpu is:    <span class="aframe-location"/> #1&#13;
We were traveling in Africa and we saw these very cute whatpus.&#13;
To do a "farduddle" means to jump up and down really fast. An example of a &#13;
sentence that uses the word farduddle is:&#13;
I was so excited that I started to farduddle.    <span class="aframe-location"/> #2&#13;
&#13;
Please only return the sentence requested by the user.  <span class="aframe-location"/> #3&#13;
&#13;
user:&#13;
{{statement}}   <span class="aframe-location"/> #4</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Demonstrates an example defining a made-up word and using it in a sentence&#13;
     <br/>#2 Demonstrates another example&#13;
     <br/>#3 A rule to prevent the LLM from outputting extra information&#13;
     <br/>#4 The input statement defines a new word and asks for the use.&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p34"> &#13;
   <p>You may say we’re forcing the LLM to hallucinate here, but this technique is the basis for modifying behavior. It allows prompts to be constructed to guide an LLM to do everything contrary to what it learned. This foundation of prompting also establishes techniques for other forms of altered behavior. From the ability to alter the perception and background of an LLM, we’ll move on to demonstrate a final example of a direct solution in the next section.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p35"> &#13;
   <h3 class="readable-text-h3" id="sigil_toc_id_139"><span class="num-string">10.1.3</span> Extracting generalities with zero-shot prompting</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p36"> &#13;
   <p><em>Zero-shot prompting or learning</em> is the ability to generate a prompt in such a manner that allows the LLM to generalize. This generalization is embedded within the LLM and demonstrated through zero-shot prompting, where no examples are given, but instead a set of guidelines or rules are given to guide the LLM.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p37"> &#13;
   <p>Employing this technique is simple and works well to guide the LLM to generate replies given its internal knowledge and no other contexts. It’s a subtle yet powerful technique that applies the knowledge of the LLM to other applications. This technique, combined with other prompting strategies, is proving effective at replacing other language classification models—models that identify the emotion or sentiment in text, for example.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p38"> &#13;
   <p>Open <code>prompt_flow/zero-shot-prompting/flow.dag.yaml</code> in the VS Code prompt flow visual editor. This flow is again almost identical to that shown earlier in figure 10.1 but differs slightly in implementation, as shown in listing 10.5.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p39"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.5</span> <code>zero-shot-prompting</code> flow</h5> &#13;
   <div class="code-area-container code-area-with-html"> &#13;
    <pre class="code-area"><strong>   </strong>Inputs:&#13;
        statement  : the statement to be classified&#13;
        expected : the expected classification of the statement&#13;
&#13;
    LLM: zero_shot (the prompt used to classify)&#13;
        inputs: statement&#13;
        outputs: the predicted class given the statement&#13;
&#13;
    Embeddings: uses an LLM embedding model to create the embedding &#13;
representation of the text&#13;
&#13;
    Embedding_predicted: embeds the output of the zero_shot LLM&#13;
    Embedding_expected: embeds the output of the expected answer&#13;
&#13;
    Python: Evaluation (Python code to measure embedding similarity)&#13;
        Inputs:&#13;
               Embedding_predicted output&#13;
             Embedding_expected output&#13;
          Outputs: the similarity score between predicted and expected&#13;
&#13;
   Outputs:&#13;
        statement: -&gt; input.statement&#13;
        expected: -&gt; input.expected&#13;
        predicted: -&gt; output.few_shot&#13;
        evaluation_score: output.evaluation&#13;
&#13;
   ### Example Output&#13;
{&#13;
       "evaluation_score": 1,    <span class="aframe-location"/> #1&#13;
       "expected": "neutral",&#13;
       "predicted": "neutral",&#13;
       "statement": "I think the vacation is okay. "    <span class="aframe-location"/> #2&#13;
   }</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Shows a perfect evaluation score of 1.0&#13;
     <br/>#2 The statement we’re asking the LLM to classify&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p40"> &#13;
   <p>Run the flow by pressing Shift-F5 within the VS Code prompt flow visual editor. You should see output similar to that shown in listing 10.5.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p41"> &#13;
   <p>Now open the <code>zero_shot.jinja2</code> prompt as shown in listing 10.6. The prompt is simple and uses no examples to extract the sentiment from the text. What is especially interesting to note is that the prompt doesn’t even mention the phrase sentiment, and the LLM seems to understand the intent.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p42"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.6</span> <code>zero_shot.jinja2</code></h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">system:&#13;
Classify the text into neutral, negative or positive. &#13;
Return on the result and nothing else.    <span class="aframe-location"/> #1&#13;
&#13;
user:&#13;
{{statement}}    <span class="aframe-location"/> #2</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Provides essential guidance on performing the classification&#13;
     <br/>#2 The statement of text to classify&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p43"> &#13;
   <p>Zero-shot prompt engineering is about using the ability of the LLM to generalize broadly based on its training material. This exercise demonstrates how knowledge within the LLM can be put to work for other tasks. The LLM’s ability to self-contextualize and apply knowledge can extend beyond its training. In the next section, we extend this concept further by looking at how LLMs can reason.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p44"> &#13;
   <h2 class="readable-text-h2" id="sigil_toc_id_140"><span class="num-string">10.2</span> Reasoning in prompt engineering</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p45"> &#13;
   <p>LLMs like ChatGPT were developed to function as chat completion models, where text content is fed into the model, whose responses align with completing that request. LLMs were never trained to reason, plan, think, or have thoughts.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p46"> &#13;
   <p>However, much like we demonstrated with the examples in the previous section, LLMs can be prompted to extract their generalities and be extended beyond their initial design. While an LLM isn’t designed to reason, the training material fed into the model provides an understanding of reasoning, planning, and thought. Therefore, by extension, an LLM understands what reasoning is and can employ the concept of reasoning.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p47"> &#13;
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Reasoning and planning</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p48"> &#13;
    <p><em>Reasoning</em> is the ability of an intellect, artificial or not, to understand the process of thought or thinking through a problem. An intellect can understand that actions have outcomes, and it can use this ability to reason through which action from a set of actions can be applied to solve a given task.</p> &#13;
   </div> &#13;
   <div class="readable-text" id="p49"> &#13;
    <p><em>Planning</em> is the ability of the intellect to reason out the order of actions or tasks and apply the correct parameters to achieve a goal or outcome—the extent to which an intellectual plan depends on the scope of the problem. An intellect may combine multiple levels of planning, from strategic and tactical to operational and contingent.</p> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p50"> &#13;
   <p>We’ll look at another set of prompt engineering techniques that allow or mimic reasoning behavior to demonstrate this reasoning ability. Typically, when evaluating the application of reasoning, we look to having the LLM solve challenging problems it wasn’t designed to solve. A good source of such is based on logic, math, and word problems.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p51"> &#13;
   <p>Using the time travel theme, what class of unique problems could be better to solve than understanding time travel? Figure 10.3 depicts one example of a uniquely challenging time travel problem. Our goal is to acquire the ability to prompt the LLM in a manner that allows it to solve the problem correctly.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p52">  &#13;
   <img alt="figure" src="Images/10-3.png" width="1012" height="589"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.3</span> The complexity of the time travel problems we intend to solve using LLMs with reasoning and planning</h5>&#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p53"> &#13;
   <p>Time travel problems are thought exercises that can be deceptively difficult to solve. The example in figure 10.3 is complicated to solve for <span class="aframe-location"/>an LLM, but the part it gets wrong may surprise you. The next section will use reasoning in prompts to solve these unique problems.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p54"> &#13;
   <h3 class="readable-text-h3" id="sigil_toc_id_141"><span class="num-string">10.2.1</span> Chain of thought prompting</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p55"> &#13;
   <p><em>Chain of thought</em> (CoT)prompting is a prompt engineering technique that employs the one-shot or few-shot examples that describe the reasoning and the steps to accomplish a desired goal. Through the demonstration of reasoning, the LLM can generalize this principle and reason through similar problems and goals. While the LLM isn’t trained with the goal of reasoning, we can elicit the model to reason, using prompt engineering.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p56"> &#13;
   <p>Open <code>prompt_flow/chain-of-thought-prompting/flow.dag.yaml</code> in the VS Code prompt flow visual editor. The elements of this flow are simple, as shown in figure 10.4. With only two LLM blocks, the flow first uses a CoT prompt to solve a complex question; then, the second LLM prompt evaluates the answer.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p57">  &#13;
   <img alt="figure" src="Images/10-4.png" width="912" height="789"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.4</span> The flow of the CoT</h5>&#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p58"> &#13;
   <p>Listing 10.7 shows the YAML pseudocode that describes the blocks and the inputs/outputs of the flow in more detail. The default problem statement in this example isn’t the same as in figure 10.3.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p59"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.7</span> <code>chain-of-thought-prompting</code> flow</h5> &#13;
   <div class="code-area-container code-area-with-html"> &#13;
    <pre class="code-area">   Inputs:&#13;
        statement  : the statement problem to be solved&#13;
        expected : the expected solution to the problem&#13;
&#13;
<strong>  </strong> LLM: cot (the prompt used to solve the problem)&#13;
        inputs: statement&#13;
        outputs: the predicted answer given the problem statement&#13;
&#13;
<strong>   </strong>LLM: evaluate_answer (the prompt used to evaluate the solution)&#13;
        inputs:&#13;
               statement: -&gt; input.statement&#13;
               expected: -&gt; input.expected&#13;
               predicted: -&gt; output.cot&#13;
&#13;
        outputs: a score of how well the problem was answered&#13;
&#13;
&#13;
   Outputs:&#13;
        statement: -&gt; input.statement&#13;
        expected: -&gt; input.expected&#13;
        predicted: -&gt; output.cot&#13;
        evaluation_score: output.evaluate_answer&#13;
&#13;
   ### Example Output&#13;
{&#13;
    "evaluation_score": "0.5",    <span class="aframe-location"/> #1&#13;
    "expected": "After the final jump, Max finds himself &#13;
in the year 1980 and he is 75 years old.",    <span class="aframe-location"/> #2&#13;
    "predicted": " Max starts in the year 2300 and &#13;
travels backward in 40-year increments, spending 5 years &#13;
in each period. The journeys will be as follows:&#13;
\n\n- From 2300 to 2260: Max is 25 + 5 = 30 years old.&#13;
\n- From 2260 to 2220: Max is 30 + 5 = 35 years old.&#13;
\n- From 2220 to 2180: Max is 35 + 5 = 40 years old.&#13;
\n- From 2180 to 2140: Max is 40 + 5 = 45 years old.&#13;
\n- From 2140 to 2100: Max is 45 + 5 = 50 years old.&#13;
\n- From 2100 to 2060: Max is 50 + 5 = 55 years old.&#13;
\n- From 2060 to 2020: Max is 55 + 5 = 60 years old.&#13;
\n- From 2020 to 1980: Max is 60 + 5 = 65 years old.&#13;
\n- From 1980 to 1940: Max is 65 + 5 = 70 years old.&#13;
\n- From 1940 to 1900: Max is 70 + 5"    <span class="aframe-location"/> #3&#13;
}</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 The evaluated score for the given solution&#13;
     <br/>#2 The expected answer for the problem&#13;
     <br/>#3 The predicted answer shows the reasoning steps and output.&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p60"> &#13;
   <p>Dig into the inputs and check the problem statement; try to evaluate the problem yourself. Then, run the flow by pressing Shift-F5. You should see output similar to that shown in listing 10.7.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p61"> &#13;
   <p>Open the <code>cot.jinja2</code> prompt file as shown in listing 10.8. This prompt gives a few examples of time travel problems and then the thought-out and reasoned solution. The process of showing the LLM the steps to complete the problem provides the reasoning mechanism.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p62"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.8</span> <code>cot.jinja2</code> </h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">system:&#13;
"In a time travel movie, Sarah travels back in time to &#13;
prevent a historic event from happening. She arrives &#13;
2 days before the event. After spending a day preparing, &#13;
she attempts to change the event but realizes she has &#13;
actually arrived 2 years early, not 2 days. She then &#13;
decides to wait and live in the past until the event's &#13;
original date. How many days does Sarah spend in the past &#13;
before the day of the event?"    <span class="aframe-location"/> #1&#13;
&#13;
Chain of Thought:    <span class="aframe-location"/> #2&#13;
&#13;
    Initial Assumption: Sarah thinks she has arrived 2 days before the event.&#13;
    Time Spent on Preparation: 1 day spent preparing.&#13;
    Realization of Error: Sarah realizes she's actually 2 years early.&#13;
    Conversion of Years to Days: &#13;
2 years = 2 × 365 = 730 days (assuming non-leap years).&#13;
    Adjust for the Day Spent Preparing: 730 - 1 = 729 days.&#13;
    Conclusion: Sarah spends 729 days in the past before the day of the event.&#13;
&#13;
"In a sci-fi film, Alex is a time traveler who decides &#13;
to go back in time to witness a famous historical battle &#13;
that took place 100 years ago, which lasted for 10 days. &#13;
He arrives three days before the battle starts. However, &#13;
after spending six days in the past, he jumps forward in &#13;
time by 50 years and stays there for 20 days. Then, he &#13;
travels back to witness the end of the battle. How many &#13;
days does Alex spend in the past before he sees the end of&#13;
 the battle?"    <span class="aframe-location"/> #3&#13;
&#13;
Chain of Thought:    <span class="aframe-location"/> #4&#13;
&#13;
    Initial Travel: Alex arrives three days before the battle starts.&#13;
    Time Spent Before Time Jump: Alex spends six days in the past. &#13;
The battle has started and has been going on for 3 days (since he &#13;
arrived 3 days early and has now spent 6 days, 3 + 3 = 6).&#13;
    First Time Jump: Alex jumps 50 years forward and stays for 20 days.&#13;
 This adds 20 days to the 6 days he's already spent in the past &#13;
(6 + 20 = 26).&#13;
    Return to the Battle: When Alex returns, he arrives back on the same &#13;
day he left (as per time travel logic). The battle has been going on for &#13;
3 days now.&#13;
    Waiting for the Battle to End: The battle lasts 10 days. Since he's &#13;
already witnessed 3 days of it, he needs to wait for 7 more days.&#13;
    Conclusion: Alex spends a total of 3 (initial wait) + 3 (before the &#13;
first jump) + 20 (50 years ago) + 7 (after returning) = 33 days in the &#13;
past before he sees the end of the battle.&#13;
Think step by step but only show the final answer to the statement.&#13;
&#13;
user:&#13;
{{statement}}    <span class="aframe-location"/> #5</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 A few example problem statements&#13;
     <br/>#2 The solution to the problem statement, output as a sequence of reasoning steps&#13;
     <br/>#3 A few example problem statements&#13;
     <br/>#4 The solution to the problem statement, output as a sequence of reasoning steps&#13;
     <br/>#5 The problem statement the LLM is directed to solve&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p63"> &#13;
   <p>You may note that the solution to figure 10.3 is also provided as an example in listing 10.8. It’s also helpful to go back and review listing 10.7 for the reply from the LLM about the problem. From this, you can see the reasoning steps the LLM applied to get its final answer.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p64"> &#13;
   <p>Now, we can look at the prompt that evaluates how well the solution solved the problem. Open <code>evaluate_answer.jinja2</code>, shown in listing 10.9, to review the prompt used. The prompt is simple, uses zero-shot prompting, and allows the LLM to generalize how it should score the expected and predicted. We could provide examples and scores, thus changing this to an example of a few-shot classification.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p65"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.9</span> <code>evaluate_answer.jinja2</code></h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">system:&#13;
&#13;
Please confirm that expected and predicted results are &#13;
the same for the given problem.    <span class="aframe-location"/> #1&#13;
Return a score from 0 to 1 where 1 is a perfect match and 0 is no match.&#13;
Please just return the score and not the explanation.    <span class="aframe-location"/> #2&#13;
&#13;
user:&#13;
Problem: {{problem}}    <span class="aframe-location"/> #3&#13;
&#13;
Expected result: {{expected}}    <span class="aframe-location"/> #4&#13;
&#13;
Predicted result: {{predicted}}    <span class="aframe-location"/> #5</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 The rules for evaluating the solution&#13;
     <br/>#2 Direction to only return the score and nothing else&#13;
     <br/>#3 The initial problem statement&#13;
     <br/>#4 The expected or grounded answer&#13;
     <br/>#5 The output from the CoT prompt earlier&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p66"> &#13;
   <p>Looking at the LLM output shown earlier in listing 10.7, you can see why the evaluation step may get confusing. Perhaps a fix to this could be suggesting to the LLM to provide the final answer in a single statement. In the next section, we move on to another example of prompt reasoning.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p67"> &#13;
   <h3 class="readable-text-h3" id="sigil_toc_id_142"><span class="num-string">10.2.2</span> Zero-shot CoT prompting</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p68"> &#13;
   <p>As our time travel demonstrates, CoT prompting can be expensive in terms of prompt generation for a specific class of problem. While not as effective, there are techniques similar to CoT that don’t use examples and can be more generalized. This section will examine a straightforward phrase employed to elicit reasoning in LLMs.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p69"> &#13;
   <p>Open <code>prompt_flow/zero-shot-cot-prompting/flow.dag.yaml</code> in the VS Code prompt flow visual editor. This flow is very similar to the previous CoT, as shown in figure 10.4. The next lsting shows the YAML pseudocode that describes the flow.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p70"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.10</span> <code>zero-shot-CoT-prompting</code> flow</h5> &#13;
   <div class="code-area-container code-area-with-html"> &#13;
    <pre class="code-area">   Inputs:&#13;
        statement  : the statement problem to be solved&#13;
        expected : the expected solution to the problem&#13;
&#13;
<strong>   </strong>LLM: cot (the prompt used to solve the problem)&#13;
        inputs: statement&#13;
        outputs: the predicted answer given the problem statement&#13;
&#13;
<strong>   </strong>LLM: evaluate_answer (the prompt used to evaluate the solution)&#13;
        inputs:&#13;
               statement: -&gt; input.statement&#13;
               expected: -&gt; input.expected&#13;
               predicted: -&gt; output.cot&#13;
&#13;
         outputs: a score of how well the problem was answered&#13;
&#13;
&#13;
    Outputs:&#13;
        statement: -&gt; input.statement&#13;
        expected: -&gt; input.expected&#13;
        predicted: -&gt; output.cot&#13;
        evaluation_score: output.evaluate_answer&#13;
&#13;
    ### Example Output&#13;
   {&#13;
       "evaluation_score": "1",    <span class="aframe-location"/> #1&#13;
       "expected": "After the final jump, <span class="">↪</span>&#13;
          <span class="">↪</span> Max finds himself in the year 1980 and &#13;
   he is 75 years old.",    <span class="aframe-location"/> #2&#13;
       "predicted": "Max starts in… <span class="">↪</span>&#13;
          <span class="">↪</span> Therefore, after the final jump, <span class="">↪</span>&#13;
          <span class="">↪</span> Max is 75 years old and in the year 1980.",    <span class="aframe-location"/> #3&#13;
       "statement": "In a complex time travel …"    <span class="aframe-location"/> #4&#13;
   }</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 The final evaluation score&#13;
     <br/>#2 The expected answer&#13;
     <br/>#3 The predicted answer (the steps have been omitted showing the final answer)&#13;
     <br/>#4 The initial problem statement&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p71"> &#13;
   <p>Run/test the flow in VS Code by pressing Shift-F5 while in the visual editor. The flow will run, and you should see output similar to that shown in listing 10.10. This exercise example performs better than the previous example on the same problem.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p72"> &#13;
   <p>Open the <code>cot.jinja2</code> prompt in VS Code, as shown in listing 10.11. This is a much simpler prompt than the previous example because it only uses zero-shot. However, one key phrase turns this simple prompt into a powerful reasoning engine. The line in the prompt <code>Let’s</code> <code>think</code> <code>step</code> <code>by</code> <code>step</code> triggers the LLM to consider internal context showing reasoning. This, in turn, directs the LLM to reason out the problem in steps.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p73"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.11</span> <code>cot.jinja2</code></h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">system:&#13;
You are an expert in solving time travel problems.&#13;
You are given a time travel problem and you have to solve it.&#13;
Let's think step by step.    <span class="aframe-location"/> #1&#13;
Please finalize your answer in a single statement.    <span class="aframe-location"/> #2&#13;
&#13;
user:&#13;
{{statement}}    <span class="aframe-location"/> #3</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 A magic line that formulates reasoning from the LLM&#13;
     <br/>#2 Asks the LLM to provide a final statement of the answer&#13;
     <br/>#3 The problem statement the LLM is asked to solve&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p74"> &#13;
   <p>Similar phrases asking the LLM to think about the steps or asking it to respond in steps also extract reasoning. We’ll demonstrate a similar but more elaborate technique in the next section.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p75"> &#13;
   <h3 class="readable-text-h3" id="sigil_toc_id_143"><span class="num-string">10.2.3</span> Step by step with prompt chaining</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p76"> &#13;
   <p>We can extend the behavior of asking an LLM to think step by step into a chain of prompts that force the LLM to solve the problem in steps. In this section, we look at a technique called <em>prompt chaining</em> that forces an LLM to process problems in steps.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p77"> &#13;
   <p>Open the <code>prompt_flow/prompt-chaining/flow.dag.yaml</code> file in the visual editor, as shown in figure 10.5. Prompt chaining breaks up the reasoning method used to solve a problem into chains of prompts. This technique forces the LLM to answer the problem in terms of steps.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p78">  &#13;
   <img alt="figure" src="Images/10-5.png" width="1100" height="958"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.5</span> The prompt chaining flow</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p79"> &#13;
   <p>Listing 10.12 shows the YAML pseudocode that describes the flow in a few more details. This flow chains the output of the first LLM block into the second and then from the second into the third. Forcing the LLM to process the problem this way uncovers the reasoning pattern, but it can also be overly verbose.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p80"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.12</span> <code>prompt-chaining</code> flow</h5> &#13;
   <div class="code-area-container code-area-with-html"> &#13;
    <pre class="code-area">   Inputs:&#13;
        statement  : the statement problem to be solved&#13;
&#13;
   LLM: decompose_steps (the prompt used to decompose the problem)&#13;
        inputs: &#13;
               statement: -&gt; input.statement    <span class="aframe-location"/> #1&#13;
&#13;
        outputs: the breakdown of steps to solve the problem&#13;
&#13;
   LLM: calculate_steps (the prompt used to calculate the steps)&#13;
        inputs:&#13;
               statement: -&gt; input.statement&#13;
               decompose_steps: -&gt; output.decompose_steps    <span class="aframe-location"/> #2&#13;
&#13;
               outputs: the calculation for each step&#13;
   LLM: calculate_solution (attempts to solve the problem)&#13;
        inputs:&#13;
               statement: -&gt; input.statement&#13;
               decompose_steps: -&gt; output.decompose_steps&#13;
               calculate_steps: -&gt; output.calculate_steps    <span class="aframe-location"/> #3&#13;
&#13;
         outputs: the final solution statement&#13;
&#13;
   Outputs:&#13;
        statement: -&gt; input.statement&#13;
        decompose_steps: -&gt; output.decompose_steps&#13;
        calculate_steps: -&gt; output.calculate_steps&#13;
        calculate_solution: -&gt; output.calculate_solution&#13;
&#13;
   ### Example Output&#13;
{&#13;
    "calculate_steps": "1. The days spent by Alex",&#13;
    "decompose_steps": "To figure out the …",&#13;
    "solution": "Alex spends 13 days in the <span class="">↪</span>&#13;
           <span class="">↪</span> past before the end of the battle.",    <span class="aframe-location"/> #4&#13;
    "statement": "In a sci-fi film, Alex …"    &#13;
}</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Start of the chain of prompts&#13;
     <br/>#2 Output from the previous step injected into this step&#13;
     <br/>#3 Output from two previous steps injected into this step&#13;
     <br/>#4 The final solution statement, although wrong, is closer.&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p81"> &#13;
   <p>Run the flow by pressing Shift-F5 from the visual editor, and you’ll see the output as shown in listing 10.12. The answer is still not correct for the Alex problem, but we can see all the work the LLM is doing to reason out the problem.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p82"> &#13;
   <p>Open up all three prompts: <code>decompose_steps.jinja2</code>, <code>calculate_steps.jinja2</code>, and <code>calculate_solution.jinja2</code> (see listings 10.13, 10.14, and 10.15, respectively). All three prompts shown in the listings can be compared to show how outputs chain together.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p83"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.13</span> <code>decompose_steps.jinja2</code></h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">system:&#13;
You are a problem solving AI assistant.&#13;
Your job is to break the users problem down into smaller steps and list &#13;
the steps in the order you would solve them.&#13;
Think step by step, not in generalities.&#13;
Do not attempt to solve the problem, just list the steps.<span class="aframe-location"/> #1&#13;
&#13;
user:&#13;
{{statement}}    <span class="aframe-location"/> #2</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Forces the LLM to list only the steps and nothing else&#13;
     <br/>#2 The initial problem statement&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p84"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.14</span> <code>calculate_steps.jinja2</code></h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">system:&#13;
You are a problem solving AI assistant.&#13;
You will be given a list of steps that solve a problem.&#13;
Your job is to calculate the output for each of the steps in order.&#13;
Do not attempt to solve the whole problem,&#13;
just list output for each of the steps.    <span class="aframe-location"/> #1&#13;
Think step by step.    <span class="aframe-location"/> #2&#13;
&#13;
user:&#13;
{{statement}}&#13;
&#13;
{{steps}}    <span class="aframe-location"/> #3</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Requests that the LLM not solve the whole problem, just the steps&#13;
     <br/>#2 Uses the magic statement to extract reasoning&#13;
     <br/>#3 Injects the steps produced by the decompose_steps step&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p85"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.15</span> <code>calculate_solution.jinja2</code></h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">system:&#13;
You are a problem solving AI assistant.&#13;
You will be given a list of steps and the calculated output for each step.&#13;
Use the calculated output from each step to determine the final &#13;
solution to the problem.&#13;
Provide only the final solution to the problem in a &#13;
single concise sentence. Do not include any steps &#13;
in your answer.    <span class="aframe-location"/> #1&#13;
&#13;
user:&#13;
{{statement}}&#13;
&#13;
{{steps}}    <span class="aframe-location"/> #2&#13;
&#13;
{{calculated}}    <span class="aframe-location"/> #3</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Requests that the LLM output the final answer and not any steps&#13;
     <br/>#2 The decomposed steps&#13;
     <br/>#3 The calculated steps&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p86"> &#13;
   <p>In this exercise example, we’re not performing any evaluation and scoring. Without the evaluation, we can see that this sequence of prompts still has problems solving our more challenging time travel problem shown earlier in figure 10.3. However, that doesn’t mean this technique doesn’t have value, and this prompting format solves some complex problems well.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p87"> &#13;
   <p>What we want to find, however, is a reasoning and planning methodology that can solve such complex problems consistently. The following section moves from reasoning to evaluating the best solution.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p88"> &#13;
   <h2 class="readable-text-h2" id="sigil_toc_id_144"><span class="num-string">10.3</span> Employing evaluation for consistent solutions</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p89"> &#13;
   <p>In the previous section, we learned that even the best-reasoned plans may not always derive the correct solution. Furthermore, we may not always have the answer to confirm if that solution is correct. The reality is that we often want to use some form of evaluation to determine the efficacy of a solution.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p90"> &#13;
   <p>Figure 10.6 shows a comparison of the prompt engineering strategies that have been devised as a means of getting LLMs to reason and plan. We’ve already covered the two on the left: zero-shot direct prompting and CoT prompting. The following example exercises in this section will look at self-consistency with the CoT and ToT techniques.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p91">  &#13;
   <img alt="figure" src="Images/10-6.png" width="1012" height="474"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.6</span> Comparing the various prompt engineering strategies to enable reasoning and planning from LLMs</h5>&#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p92"> &#13;
   <p>We’ll continue to focus on the complex time travel problem to compare these more advanced methods that expand on reasoning and planning with evaluation. In the next section, we’ll evaluate self-consistency.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p93"> &#13;
   <h3 class="readable-text-h3" id="sigil_toc_id_145"><span class="num-string">10.3.1</span> Evaluating self-consistency prompting</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p94"> &#13;
   <p>Consistency in prompting is more than just lowering the temperature parameter we send to an LLM. Often, we want to generate a consistent plan or solution and still use a high temperature to better evaluate all the variations to a plan. By evaluating multiple different plans, we can get a better sense of the overall value of a solution.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p95"> &#13;
   <p><em>Self-consistent prompting</em> is the technique of generating multiple plans/solutions for a given problem. Then, those plans are evaluated, and the more frequent or consistent plan is accepted. Imagine three plans generated, where two are similar, but the third is different. Using self-consistency, we evaluate the first two plans as the more consistent answer.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p96"> &#13;
   <p>Open <code>prompt_flow/self-consistency-prompting/flow.dag.yaml</code> in the VS Code prompt flow visual editor. The flow diagram shows the simplicity of the prompt generation flow in figure 10.7. Next to it in the diagram is the self-consistency evaluation flow.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p97">  &#13;
   <img alt="figure" src="Images/10-7.png" width="1009" height="729"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.7</span> The self-consistency prompt generation beside the evaluation flow</h5>&#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p98"> &#13;
   <p>Prompt flow uses a direct acyclic graph (DAG) format to execute the flow logic. DAGs are an excellent way of demonstrating and executing flow logic, but because they are <em>acyclic,</em> meaning they can’t repeat, they can’t execute loops. However, because prompt flow provides a batch processing mechanism, we can use that to simulate loops or repetition in a flow.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p99"> &#13;
   <p>Referring to figure 10.6, we can see that self-consistency processes the input three times before collecting the results and determining the best plan/reply. We can apply this same pattern but use batch processing to generate the outputs. Then, the evaluation flow will aggregate the results and determine the best answer.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p100"> &#13;
   <p>Open the <code>self-consistency-prompting/cot.jinja2</code> prompt template in VS Code (see listing 10.16). The listing was shortened, as we’ve seen parts before. This prompt uses two (few-shot prompt) examples of a CoT to demonstrate the thought reasoning to the LLM.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p101"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.16</span> <code>self-consistency-prompting/cot.jinja2</code></h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">system:&#13;
&#13;
"In a time travel movie, Sarah travels back… "    <span class="aframe-location"/> #1&#13;
&#13;
Chain of Thought:&#13;
&#13;
    Initial Assumption: …    <span class="aframe-location"/> #2&#13;
    Conclusion: Sarah spends 729 days in the past before the day of the event.&#13;
&#13;
"In a complex time travel movie plot, Max, a 25 year old…"    <span class="aframe-location"/> #3&#13;
&#13;
Chain of Thought:&#13;
    Starting Point: Max starts …    <span class="aframe-location"/> #4&#13;
    Conclusion: After the final jump, &#13;
Max finds himself in the year 1980 and he is 75 years old.&#13;
Think step by step,&#13;
 but only show the final answer to the statement.    <span class="aframe-location"/> #5&#13;
&#13;
user:&#13;
{{statement}}</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 The Sarah time travel problem&#13;
     <br/>#2 Sample CoT, cut for brevity&#13;
     <br/>#3 The Max time travel problem&#13;
     <br/>#4 Sample CoT, cut for brevity&#13;
     <br/>#5 Final guide and statement to constrain output&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p102"> &#13;
   <p>Open the <code>self-consistency-prompting/flow.dag.yaml</code> file in VS Code. Run the example in batch mode by clicking Batch Run (the beaker icon) from the visual editor. Figure 10.8 shows the process step by step: </p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p103"> Click Batch Run. </li> &#13;
   <li class="readable-text" id="p104"> Select the JSON Lines (JSONL) input. </li> &#13;
   <li class="readable-text" id="p105"> Select <code>statements.jsonl</code>. </li> &#13;
   <li class="readable-text" id="p106"> Click the Run link.<span class="aframe-location"/> </li> &#13;
  </ol> &#13;
  <div class="browsable-container figure-container" id="p107">  &#13;
   <img alt="figure" src="Images/10-8.png" width="927" height="774"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.8</span> The step-by-step process of launching a batch process</h5>&#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p108"> &#13;
   <p><span class="print-book-callout-head">TIP</span>  If you need to review the process, refer to chapter 9, which covers this process in more detail.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p109"> &#13;
   <p>Listing 10.17 shows the JSON output from executing the flow in batch mode. The <code>statements.jsonl</code> file has five identical Alex time travel problem entries. Using identical entries allows us to simulate the prompt executing five times on the duplicate entry.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p110"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.17</span> <code>self-consistency-prompting</code> batch execution output</h5> &#13;
   <div class="code-area-container code-area-with-html"> &#13;
    <pre class="code-area">{&#13;
    "name": "self-consistency-prompting_default_20240203_100322_912000",&#13;
    "created_on": "2024-02-03T10:22:30.028558",&#13;
    "status": "Completed",&#13;
    "display_name": "self-consistency-prompting_variant_0_202402031022",&#13;
    "description": null,&#13;
    "tags": null,&#13;
    "properties": {&#13;
        "flow_path": "…prompt_flow/self-consistency-prompting",    <span class="aframe-location"/> #1&#13;
        <strong>"output_path"</strong>: "…/.promptflow/.runs/self-&#13;
<span class="">↪</span> consistency-prompting_default_20240203_100322_912000",    <span class="aframe-location"/> #2&#13;
        "system_metrics": {&#13;
            "total_tokens": 4649,&#13;
            "prompt_tokens": 3635,&#13;
            "completion_tokens": 1014,&#13;
            "duration": 30.033773&#13;
        }&#13;
    },&#13;
    "flow_name": "self-consistency-prompting",&#13;
    "data": "…/prompt_flow/self-consistency-prompting/&#13;
<span class="">↪</span> statements.jsonl",    <span class="aframe-location"/> #3&#13;
    "output": "…/.promptflow/.runs/self-consistency-<span class="">↪</span>&#13;
<span class="">↪</span> prompting_default_20240203_100322_912000/flow_outputs"&#13;
}</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 The path where the flow was executed from&#13;
     <br/>#2 The folder containing the outputs of the flow (note this path)&#13;
     <br/>#3 The data used to run the flow in batch&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p111"> &#13;
   <p>You can view the flow produced by pressing the Ctrl key and clicking the output link, highlighted in listing 10.17. This will open another instance of VS Code, showing a folder with all the output from the run. We now want to check the most consistent answer. Fortunately, the evaluation feature in prompt flow can help us identify consistent answers using similarity matching.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p112"> &#13;
   <p>Open <code>self-consistency-evaluation/flow.dag.yaml</code> in VS Code (see figure 10.7). This flow embeds the predicted answer and then uses an aggregation to determine the most consistent answer.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p113"> &#13;
   <p>From the flow, open <code>consistency.py</code> in VS Code, as shown in listing 10.18. The code for this tool function calculates the cosine similarity for all pairs of answers. Then, it finds the most similar answer, logs it, and outputs that as the answer.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p114"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.18</span> <code>consistency.py</code></h5> &#13;
   <div class="code-area-container code-area-with-html"> &#13;
    <pre class="code-area">from promptflow import tool&#13;
from typing import List&#13;
import numpy as np&#13;
from scipy.spatial.distance import cosine&#13;
@tool&#13;
def consistency(texts: List[str],&#13;
                embeddings: List[List[float]]) -&gt; str:&#13;
    if len(embeddings) != len(texts):&#13;
        raise ValueError("The number of embeddings <span class="">↪</span>&#13;
       <span class="">↪</span> must match the number of texts.")&#13;
&#13;
    mean_embedding = np.mean(embeddings, axis=0)    <span class="aframe-location"/> #1&#13;
    similarities = [1 - cosine(embedding, mean_embedding) <span class="">↪</span>&#13;
                <span class="">↪</span> for embedding in embeddings]    <span class="aframe-location"/> #2&#13;
    most_similar_index = np.argmax(similarities)    <span class="aframe-location"/> #3&#13;
&#13;
    from promptflow import log_metric&#13;
    log_metric(key="highest_ranked_output", value=texts[most_similar_index])    <span class="aframe-location"/> #4&#13;
&#13;
    return texts[most_similar_index]    <span class="aframe-location"/> #5</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Calculates the mean of all the embeddings&#13;
     <br/>#2 Calculates cosine similarity for each pair of embeddings&#13;
     <br/>#3 Finds the index of the most similar answer&#13;
     <br/>#4 Logs the output as a metric&#13;
     <br/>#5 Returns the text for the most similar answer&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p115"> &#13;
   <p>We need to run the evaluation flow in batch mode as well. Open <code>self-consistency-evaluation/flow.dag.yaml</code> in VS Code and run the flow in batch mode (beaker icon). Then, select Existing Run as the flow input, and when prompted, choose the top or the last run you just executed as input.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p116"> &#13;
   <p>Again, after the flow completes processing, you’ll see an output like that shown in listing 10.17. Ctrl-click on the output folder link to open a new instance of VS Code showing the results. Locate and open the <code>metric.json</code> file in VS Code, as shown in figure 10.9.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p117">  &#13;
   <img alt="figure" src="Images/10-9.png" width="1012" height="682"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.9</span> The VS Code is open to the batch run output folder. Highlighted are the <code>metrics.json</code> file and the output showing the most similar answer.</h5>&#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p118"> &#13;
   <p>The answer shown in figure 10.9 is still incorrect for this run. You can continue a few more batch runs of the prompt and/or increase the number of runs in a batch and then evaluate flows to see if you get better answers. This technique is generally more helpful for more straightforward problems but still demonstrates an inability to reason out complex problems.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p119"> &#13;
   <p>Self-consistency uses a reflective approach to evaluate the most likely thought. However, the most likely thing is certainly not always the best. Therefore, we must consider a more comprehensive approach in the next section.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p120"> &#13;
   <h3 class="readable-text-h3" id="sigil_toc_id_146"><span class="num-string">10.3.2</span> Evaluating tree of thought prompting</h3> &#13;
  </div> &#13;
  <div class="readable-text" id="p121"> &#13;
   <p>As mentioned earlier, ToT prompting, as shown in figure 10.6, combines self-evaluation and prompt chaining techniques. As such, it breaks down the sequence of planning into a chain of prompts, but at each step in the chain, it provides for multiple evaluations. This creates a tree that can be executed and evaluated at each level, breadth-first, or from top to bottom, depth-first.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p122"> &#13;
   <p>Figure 10.10 shows the difference between executing a tree using breadth-first or depth-first. Unfortunately, due to the DAG execution pattern of prompt flow, we can’t quickly implement the depth-first method, but breadth-first works just fine.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p123">  &#13;
   <img alt="figure" src="Images/10-10.png" width="1100" height="699"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.10</span> Breadth-first vs. depth-first execution on a ToT pattern</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p124"> &#13;
   <p>Open <code>tree-of-thought-evaluation/flow.dag.yaml</code> in VS Code. The visual of the flow is shown in figure 10.11. This flow functions like a breadth-first ToT pattern—the flow chains together a series of prompts asking the LLM to return multiple plans at each step.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p125">  &#13;
   <img alt="figure" src="Images/10-11.png" width="1012" height="659"/> &#13;
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.11</span> ToT pattern expressed and prompt flow</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p126"> &#13;
   <p>Because the flow executes in a breadth-first style, each level output of the nodes is also evaluated. Each node in the flow uses a pair of semantic functions—one to generate the answer and the other to evaluate the answer. The semantic function is a custom Python flow block that processes multiple inputs and generates multiple outputs.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p127"> &#13;
   <p>Listing 10.19 shows the <code>semantic_function.py</code> tool. This general tool is reused for multiple blocks in this flow. It also demonstrates the embedding functionality from the SK for direct use within prompt flow.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p128"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.19</span> <code>semantic_function.py</code></h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">@tool&#13;
def my_python_tool(&#13;
    input: str,&#13;
    input_node: int,&#13;
    history: str,&#13;
    semantic_function: str,&#13;
    evaluation_function: str,&#13;
    function_name: str,&#13;
    skill_name: str,&#13;
    max_tokens: int,&#13;
    temperature: float,&#13;
    deployment_name: str,&#13;
    connection: Union[OpenAIConnection, &#13;
                      AzureOpenAIConnection],    <span class="aframe-location"/> #1&#13;
) -&gt; str:&#13;
    if input is None or input == "":    <span class="aframe-location"/> #2&#13;
        return ""&#13;
&#13;
    kernel = sk.Kernel(log=sk.NullLogger())&#13;
    # code for setting up the kernel and LLM connection omitted&#13;
&#13;
&#13;
    function = kernel.create_semantic_function(&#13;
                             semantic_function,                                               &#13;
                             function_name=function_name,&#13;
                             skill_name=skill_name,&#13;
                             max_tokens=max_tokens,&#13;
                             temperature=temperature,&#13;
                             top_p=0.5)    <span class="aframe-location"/> #3&#13;
    evaluation = kernel.create_semantic_function(&#13;
                             evaluation_function,        &#13;
                             function_name="Evaluation",&#13;
                             skill_name=skill_name,&#13;
                             max_tokens=max_tokens,&#13;
                             temperature=temperature,&#13;
                             top_p=0.5)    <span class="aframe-location"/> #4&#13;
&#13;
    async def main():&#13;
        query = f"{history}\n{input}"&#13;
        try:&#13;
            eval = int((await evaluation.invoke_async(query)).result)&#13;
            if eval &gt; 25:    <span class="aframe-location"/> #5&#13;
                return await function.invoke_async(query)   <span class="aframe-location"/> #6&#13;
        except Exception as e:&#13;
            raise Exception("Evaluation failed", e)&#13;
&#13;
       try:&#13;
        result = asyncio.run(main()).result&#13;
        return result&#13;
    except Exception as e:&#13;
        print(e)&#13;
        return ""</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Uses a union to allow for different types of LLM connections&#13;
     <br/>#2 Checks to see if the input is empty or None; if so, the function shouldn’t be executed.&#13;
     <br/>#3 Sets up the generation function that creates a plan&#13;
     <br/>#4 Sets up the evaluation function&#13;
     <br/>#5 Runs the evaluate function and determines if the input is good enough to continue&#13;
     <br/>#6 If the evaluation score is high enough, generates the next step&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p129"> &#13;
   <p>The semantic function tool is used in the tree’s experts, nodes, and answer blocks. At each step, the function determines if any text is being input. If there is no text, the block returns with no execution. Passing no text to a block means that the previous block failed evaluation. By evaluating before each step, ToT short-circuits the execution of plans it deems as not being valid.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p130"> &#13;
   <p>This may be a complex pattern to grasp at first, so go ahead and run the flow in VS Code. Listing 10.20 shows just the answer node output of a run; these results may vary from what you see but should be similar. Nodes that return no text either failed evaluation or their parents did.</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p131"> &#13;
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.20</span> Output from <code>tree-of-thought-evaluation</code> flow</h5> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">{&#13;
    "answer_1_1": "",    <span class="aframe-location"/> #1&#13;
    "answer_1_2": "",&#13;
    "answer_1_3": "",&#13;
    "answer_2_1": "Alex spends a total of 29 days in the past before he &#13;
sees the end of the battle.",&#13;
    "answer_2_2": "",    <span class="aframe-location"/> #2&#13;
    "answer_2_3": "Alex spends a total of 29 days in the past before he &#13;
sees the end of the battle.",&#13;
    "answer_3_1": "",    <span class="aframe-location"/> #3&#13;
    "answer_3_2": "Alex spends a total of 29 days in the past before he &#13;
sees the end of the battle.",&#13;
    "answer_3_3": "Alex spends a total of 9 days in the past before he &#13;
sees the end of the battle.",</pre> &#13;
    <div class="code-annotations-overlay-container">&#13;
     #1 Represents that the first node plans weren’t valid and not executed&#13;
     <br/>#2 The plan for node 2 and answer 2 failed evaluation and wasn’t run.&#13;
     <br/>#3 The plan for this node failed to evaluate and wasn’t run.&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p132"> &#13;
   <p>The output in listing 10.20 shows how only a select set of nodes was evaluated. In most cases, the evaluated nodes returned an answer that could be valid. Where no output was produced, it means that the node itself or its parent wasn’t valid. When sibling nodes all return empty, the parent node fails to evaluate.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p133"> &#13;
   <p>As we can see, ToT is valid for complex problems but perhaps not very practical. The execution of this flow can take up to 27 calls to an LLM to generate an output. In practice, it may only do half that many calls, but that’s still a dozen or more calls to answer a single problem.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p134"> &#13;
   <h2 class="readable-text-h2" id="sigil_toc_id_147"><span class="num-string">10.4</span> Exercises</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p135"> &#13;
   <p>Use the following exercises to improve your knowledge of the material:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p136"> <em>Exercise 1</em>—Create Direct Prompting, Few-Shot Prompting, and Zero-Shot Prompting </li> &#13;
  </ul> &#13;
  <div class="readable-text list-body-item" id="p137"> &#13;
   <p><em>Objective </em>—Create three different prompts for an LLM to summarize a recent scientific article: one using direct prompting, one with few-shot prompting, and the last employing zero-shot prompting. </p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p138"> &#13;
   <p><em>Tasks:</em></p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class=" buletless-item" style="list-style-type: none;"> &#13;
    <ul> &#13;
     <li class="readable-text" id="p139"> Compare the effectiveness of the summaries generated by each approach. </li> &#13;
     <li class="readable-text" id="p140"> Compare the accuracy of the summaries generated by each approach. </li> &#13;
    </ul></li> &#13;
   <li class="readable-text" id="p141"> <em>Exercise 2</em>—Craft Reasoning Prompts </li> &#13;
  </ul> &#13;
  <div class="readable-text list-body-item" id="p142"> &#13;
   <p><em>Objective </em>—Design a set of prompts that require the LLM to solve logical puzzles or riddles.</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p143"> &#13;
   <p><em>Tasks:</em></p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class=" buletless-item" style="list-style-type: none;"> &#13;
    <ul> &#13;
     <li class="readable-text" id="p144"> Focus on how the structure of your prompt can influence the LLM’s reasoning process. </li> &#13;
     <li class="readable-text" id="p145"> Focus on how the same can influence the correctness of its answers. </li> &#13;
    </ul></li> &#13;
   <li class="readable-text" id="p146"> <em>Exercise 3</em>—Evaluation Prompt Techniques </li> &#13;
  </ul> &#13;
  <div class="readable-text list-body-item" id="p147"> &#13;
   <p><em>Objective </em>—Develop an evaluation prompt that asks the LLM to predict the outcome of a hypothetical experiment.</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p148"> &#13;
   <p><em>Task:</em></p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class=" buletless-item" style="list-style-type: none;"> &#13;
    <ul> &#13;
     <li class="readable-text" id="p149"> Create a follow-up prompt that evaluates the LLM’s prediction for accuracy and provides feedback on its reasoning process. </li> &#13;
    </ul></li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p150"> &#13;
   <h2 class="readable-text-h2" id="sigil_toc_id_148">Summary</h2> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p151"> Direct solution prompting is a foundational method of using prompts to direct LLMs toward solving specific problems or tasks, emphasizing the importance of clear question-and-answer structures. </li> &#13;
   <li class="readable-text" id="p152"> Few-shot prompting provides LLMs with a few examples to guide them in handling new or unseen content, highlighting its power in enabling the model to adapt to unfamiliar patterns. </li> &#13;
   <li class="readable-text" id="p153"> Zero-shot learning and prompting demonstrate how LLMs can generalize from their training to solve problems without needing explicit examples, showcasing their inherent ability to understand and apply knowledge in new contexts. </li> &#13;
   <li class="readable-text" id="p154"> Chain of thought prompting guides the LLMs through a reasoning process step by step to solve complex problems, illustrating how to elicit detailed reasoning from the model. </li> &#13;
   <li class="readable-text" id="p155"> Prompt chaining breaks down a problem into a series of prompts that build upon each other, showing how to structure complex problem-solving processes into manageable steps for LLMs. </li> &#13;
   <li class="readable-text" id="p156"> Self-consistency is a prompt technique that generates multiple solutions to a problem and selects the most consistent answer through evaluation, emphasizing the importance of consistency in achieving reliable outcomes. </li> &#13;
   <li class="readable-text" id="p157"> Tree of thought prompting combines self-evaluation and prompt chaining to create a comprehensive strategy for tackling complex problems, allowing for a systematic exploration of multiple solution paths. </li> &#13;
   <li class="readable-text" id="p158"> Advanced prompt engineering strategies provide insights into sophisticated techniques such as self-consistency with CoT and ToT, offering methods to increase the accuracy and reliability of LLM-generated solutions.<span class=" link-like"/> </li> &#13;
  </ul>&#13;
 </div></body>
</html>