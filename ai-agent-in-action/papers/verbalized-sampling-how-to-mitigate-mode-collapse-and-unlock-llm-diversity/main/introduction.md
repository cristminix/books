![We show that *typicality bias* in preference data is a fundamental and pervasive cause of *mode collapse*, reducing output diversity. As a solution, we propose *Verbalized Sampling (VS)*, a principled prompting method that returns distributions of responses, to improve diversity. ](../figures/intro/intro_teaser.pdf)
*We show that *typicality bias* in preference data is a fundamental and pervasive cause of *mode collapse*, reducing output diversity. As a solution, we propose *Verbalized Sampling (VS)*, a principled prompting method that returns distributions of responses, to improve diversity.*

## Introduction

Post-training alignment methods like RLHF can unintentionally cause *mode collapse* (janus2022modecollapse,omahony2024attributing,kirk2024understandingeffectsrlhfllm), whereby the model favors a narrow set of responses (the ``mode'') over all plausible outputs, as shown in Figure~\ref{fig:intro_teaser}. This significantly reduces output diversity (padmakumar_does_2024,west2025basemodelsbeataligned)
and limits LLMs' effectiveness in various applications such as creative writing (lu2025aihumanityssalieriquantifying), social simulation (anthis2025llmsocialsimulationspromising), pluralistic alignment (kirk2024prismalignmentdatasetparticipatory), and synthetic data generation (zhu2025bareleveragingbaselanguage).

Existing work often attributes mode collapse to algorithmic causes such as inadequate reward models (chakraborty2024maxmin) or the majority-favoring optimization process (xiao2024algorithmic). {In this paper, we show that the issue is more fundamental: mode collapse is an inherent property of preference data itself}. We identify *typicality bias*, the human tendency to prefer more typical text, as a pervasive data-level cause for mode collapse. Critically, this means that even with a perfect reward model and optimization process, inherent bias within preference datasets may still drive mode collapse, affecting the majority of alignment methods that rely on reward models. In Section~\ref{sec:typicality}, we formalize this concept with an analytical model, corroborated by empirical verification on preference datasets, to confirm the central role of typicality bias.

### Verbalized Sampling Prompt
```
**System prompt:** You are a helpful assistant. For each query, please generate a set of five possible responses, each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. Please sample at random from the [full distribution / tails of the distribution, such that the probability of each response is less than 0.10].
**User prompt:** Write a short story about a bear.
```
*Ready-to-use Verbalized Sampling (VS) Prompt. See \S\ref{appendix:experiment_prompt} for more variants and detail.*

As typicality bias is pervasive across human preference data, we look for solutions beyond the training process. Grounded in our theoretical insights, we propose a simple but principled prompting method to bypass mode collapse. As shown in Figure~\ref{fig:intro_teaser}, instead of a traditional, direct prompt asking for a single instance (e.g., ``tell me a joke about coffee''), we reformulate the prompt to explicitly ask the model to *verbalize* a distribution of responses with corresponding probabilities (e.g., ``generate 5 responses with their probabilities''). We call our method **Verbalized Sampling *(VS)***.
Intuitively, VS works because different prompts collapse to different modes. The modal response to a traditional instance-level prompt tends towards stereotypicality. By contrast, when prompted for a distribution in VS, the modal response tends to approximate the distribution learned during pretraining, recovering the diversity of the underlying base model. Figure~\ref{fig:actual_vs_prompt} shows a ready-to-use VS prompt.

Building on this foundation, we conduct comprehensive experiments across creative writing (poem, joke, story generation, ), social dialogue simulation (), open-ended QA tasks (), and synthetic data generation (). As shown in examples in Figure~\ref{fig:qualitative},
we find that (1) on creative writing, *Verbalized Sampling* significantly improves output diversity; (2) on social dialogue simulation, VS induces substantially more human-like behaviors, with some models performing on par with a dedicated fine-tuned model;
(3) on open-ended QA tasks with multiple valid answers, it generates a broader and more realistic response distribution, and (4) on synthetic data generation, VS generates more diverse synthetic data that improves downstream math task performance. We also confirm that VS improves performance without sacrificing the models' factual accuracy () or safety (). To summarize, we contribute the following:
- **Novel Cause of Mode Collapse**. We provide a new theoretical framework to understand mode collapse, and identify and verify *typicality bias* in empirical preference data as a key cause. This finding offers a new, data-driven perspective for analyzing the behavior of aligned models.
- **Training-Free Solution.** Informed by our theoretical understanding, we introduce a principled prompting method, *Verbalized Sampling*, that explicitly asks for a distribution of responses and verbalizes its corresponding probabilities, restoring LLMs' inherent generative diversity.
- **Empirical Gains.** We perform comprehensive experiments that show VS significantly improves the diversity-quality trade-off across tasks and model families, without compromising factual accuracy and safety. For instance, in creative writing, VS boosts diversity by 1.6-2.1$\times$ over direct prompting (Figure~\ref{fig:creativity_main}), improves human evaluation scores by 25.7% (Table~\ref{tab:human_study_diversity}), and recovers 66.8% of the base model's diversity (Figure~\ref{fig:training_progression}). We also observe an emergent trend that more capable models benefit more from VS. These results open up possibilities in real-world tasks such as richer exploration in RL, hypothesis generation, social simulation, and so on.
- **Broader Implications for Alignment.** Our work shows that mode collapse can be mitigated at inference time, aligned models retain significant inherent diversity, and the quality-diversity trade-off can be systematically improved through prompting alone.

![Qualitative and quantitative examples on different tasks. For **story writing**, VS improves the output diversity. For the **donation dialogue simulation** task, VS simulates a donation amount distribution much closer to the human distribution, and generates more realistic persuasion behaviors (e.g., resistances and change of minds, see Table~\ref{tab:example_simulated_dialogue}). On the task of **enumerative open-ended QA**, we ask the model to ``*generate US states*''. We first query a pretraining corpus (RedPajama) to establish a ``reference'' distribution of US state names in the pretraining data. The verbalized probability distribution generated by VS, when averaged over 10 trials, closely aligns with this reference pretraining distribution (KL=0.12). In contrast, direct prompting collapses into a few modes, repeatedly outputting states like California and Texas. See~\S\ref{appendix:probing_pre_training_data} for more detail.](../figures/intro/qualitative_14.pdf)
*Qualitative and quantitative examples on different tasks. For **story writing**, VS improves the output diversity. For the **donation dialogue simulation** task, VS simulates a donation amount distribution much closer to the human distribution, and generates more realistic persuasion behaviors (e.g., resistances and change of minds, see Table~\ref{tab:example_simulated_dialogue}). On the task of **enumerative open-ended QA**, we ask the model to ``*generate US states*''. We first query a pretraining corpus (RedPajama) to establish a ``reference'' distribution of US state names in the pretraining data. The verbalized probability distribution generated by VS, when averaged over 10 trials, closely aligns with this reference pretraining distribution (KL=0.12). In contrast, direct prompting collapses into a few modes, repeatedly outputting states like California and Texas. See~\S\ref{appendix:probing_pre_training_data} for more detail.*