![Kami menunjukkan bahwa *bias tipikalitas* dalam data preferensi adalah penyebab mendasar dan meluas dari *keruntuhan mode*, mengurangi keragaman keluaran. Sebagai solusi, kami mengusulkan *Verbalized Sampling (VS)*, sebuah metode pemicuan berprinsip yang mengembalikan distribusi respons, untuk meningkatkan keragaman. ](../figures/intro/intro_teaser.png)
*Kami menunjukkan bahwa *bias tipikalitas* dalam data preferensi adalah penyebab mendasar dan meluas dari *keruntuhan mode*, mengurangi keragaman keluaran. Sebagai solusi, kami mengusulkan *Verbalized Sampling (VS)_, sebuah metode pemicuan berprinsip yang mengembalikan distribusi respons, untuk meningkatkan keragaman._

## Pendahuluan

Metode penyelarasan pasca-pelatihan seperti RLHF secara tidak sengaja dapat menyebabkan _keruntuhan mode_ (janus2022modecollapse,omahony2024attributing,kirk2024understandingeffectsrlhfllm), di mana model lebih menyukai serangkaian respons yang sempit (``mode'') daripada semua keluaran yang masuk akal, seperti yang ditunjukkan pada Gambar~\ref{fig:intro_teaser}. Hal ini secara signifikan mengurangi keragaman keluaran (padmakumar_does_2024,west2025basemodelsbeataligned)
dan membatasi efektivitas LLM dalam berbagai aplikasi seperti penulisan kreatif (lu2025aihumanityssalieriquantifying), simulasi sosial (anthis2025llmsocialsimulationspromising), penyelarasan pluralistik (kirk2024prismalignmentdatasetparticipatory), dan pembuatan data sintetis (zhu2025bareleveragingbaselanguage).

Karya yang ada sering mengaitkan keruntuhan mode dengan penyebab algoritmik seperti model hadiah yang tidak memadai (chakraborty2024maxmin) atau proses optimasi yang menguntungkan mayoritas (xiao2024algorithmic). {Dalam makalah ini, kami menunjukkan bahwa masalahnya lebih mendasar: keruntuhan mode adalah properti inheren dari data preferensi itu sendiri}. Kami mengidentifikasi _bias tipikalitas_, kecenderungan manusia untuk lebih menyukai teks yang lebih tipikal, sebagai penyebab tingkat data yang meluas untuk keruntuhan mode. Secara kritis, ini berarti bahwa bahkan dengan model hadiah dan proses optimasi yang sempurna, bias inheren dalam kumpulan data preferensi mungkin masih mendorong keruntuhan mode, memengaruhi sebagian besar metode penyelarasan yang mengandalkan model hadiah. Dalam Bagian~\ref{sec:typicality}, kami memformalkan konsep ini dengan model analitis, yang dikuatkan oleh verifikasi empiris pada kumpulan data preferensi, untuk mengkonfirmasi peran sentral bias tipikalitas.

### Prompt Verbalized Sampling

```
**Prompt Sistem:** Anda adalah asisten yang membantu. Untuk setiap kueri, harap hasilkan lima kemungkinan respons, masing-masing dalam tag <response> terpisah. Setiap respons harus menyertakan <text> dan <probability> numerik. Harap ambil sampel secara acak dari [distribusi penuh / ekor distribusi, sedemikian rupa sehingga probabilitas setiap respons kurang dari 0.10].
**Prompt Pengguna:** Tulis cerita pendek tentang beruang.
```

Original English Text:

```
**System prompt:** You are a helpful assistant. For each query, please generate a set of five possible responses, each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. Please sample at random from the [full distribution / tails of the distribution, such that the probability of each response is less than 0.10].
**User prompt:** Write a short story about a bear.
```

_Prompt Verbalized Sampling (VS) yang siap digunakan. Lihat \S\ref{appendix:experiment_prompt} untuk varian dan detail lebih lanjut._

Karena bias tipikalitas meluas di seluruh data preferensi manusia, kami mencari solusi di luar proses pelatihan. Berdasarkan wawasan teoretis kami, kami mengusulkan metode pemicuan yang sederhana namun berprinsip untuk melewati keruntuhan mode. Seperti yang ditunjukkan pada Gambar~\ref{fig:intro*teaser}, alih-alih prompt tradisional langsung yang meminta satu instans (misalnya, `ceritakan lelucon tentang kopi''), kami memformulasikan ulang prompt untuk secara eksplisit meminta model untuk *memverbalisasi* distribusi respons dengan probabilitas yang sesuai (misalnya, `hasilkan 5 respons dengan probabilitasnya''). Kami menyebut metode kami \*\*Verbalized Sampling *(VS)\_\*\*.
Secara intuitif, VS bekerja karena prompt yang berbeda runtuh ke mode yang berbeda. Respons modal terhadap prompt tingkat instans tradisional cenderung ke arah stereotip. Sebaliknya, ketika diminta untuk distribusi dalam VS, respons modal cenderung mendekati distribusi yang dipelajari selama pra-pelatihan, memulihkan keragaman model dasar yang mendasarinya. Gambar~\ref{fig:actual_vs_prompt} menunjukkan prompt VS yang siap digunakan.

Berdasarkan fondasi ini, kami melakukan eksperimen komprehensif di seluruh penulisan kreatif (puisi, lelucon, pembuatan cerita, ), simulasi dialog sosial (), tugas QA terbuka (), dan pembuatan data sintetis (). Seperti yang ditunjukkan dalam contoh pada Gambar~\ref{fig:qualitative},
kami menemukan bahwa (1) pada penulisan kreatif, _Verbalized Sampling_ secara signifikan meningkatkan keragaman keluaran; (2) pada simulasi dialog sosial, VS menginduksi perilaku yang jauh lebih mirip manusia, dengan beberapa model berkinerja setara dengan model yang disetel secara khusus;
(3) pada tugas QA terbuka dengan banyak jawaban yang valid, ia menghasilkan distribusi respons yang lebih luas dan lebih realistis, dan (4) pada pembuatan data sintetis, VS menghasilkan data sintetis yang lebih beragam yang meningkatkan kinerja tugas matematika hilir. Kami juga mengkonfirmasi bahwa VS meningkatkan kinerja tanpa mengorbankan akurasi faktual model () atau keamanan (). Untuk meringkas, kami berkontribusi sebagai berikut:

- **Penyebab Baru Keruntuhan Mode**. Kami menyediakan kerangka kerja teoretis baru untuk memahami keruntuhan mode, dan mengidentifikasi serta memverifikasi _bias tipikalitas_ dalam data preferensi empiris sebagai penyebab utama. Temuan ini menawarkan perspektif baru yang didorong oleh data untuk menganalisis perilaku model yang selaras.
- **Solusi Bebas Pelatihan.** Berdasarkan pemahaman teoretis kami, kami memperkenalkan metode pemicuan berprinsip, _Verbalized Sampling_, yang secara eksplisit meminta distribusi respons dan memverbalisasi probabilitas yang sesuai, memulihkan keragaman generatif inheren LLM.
- **Keuntungan Empiris.** Kami melakukan eksperimen komprehensif yang menunjukkan bahwa VS secara signifikan meningkatkan trade-off keragaman-kualitas di seluruh tugas dan keluarga model, tanpa mengorbankan akurasi faktual dan keamanan. Misalnya, dalam penulisan kreatif, VS meningkatkan keragaman sebesar 1.6-2.1$\times$ dibandingkan pemicuan langsung (Gambar~\ref{fig:creativity_main}), meningkatkan skor evaluasi manusia sebesar 25.7% (Tabel~\ref{tab:human_study_diversity}), dan memulihkan 66.8% keragaman model dasar (Gambar~\ref{fig:training_progression}). Kami juga mengamati tren yang muncul bahwa model yang lebih mampu lebih banyak mendapat manfaat dari VS. Hasil ini membuka kemungkinan dalam tugas dunia nyata seperti eksplorasi yang lebih kaya dalam RL, pembuatan hipotesis, simulasi sosial, dan sebagainya.
- **Implikasi yang Lebih Luas untuk Penyelarasan.** Karya kami menunjukkan bahwa keruntuhan mode dapat dimitigasi pada waktu inferensi, model yang selaras mempertahankan keragaman inheren yang signifikan, dan trade-off kualitas-keragaman dapat ditingkatkan secara sistematis melalui pemicuan saja.

![Contoh kualitatif dan kuantitatif pada tugas yang berbeda. Untuk **penulisan cerita**, VS meningkatkan keragaman keluaran. Untuk tugas **simulasi dialog donasi**, VS mensimulasikan distribusi jumlah donasi yang jauh lebih dekat dengan distribusi manusia, dan menghasilkan perilaku persuasi yang lebih realistis (misalnya, resistensi dan perubahan pikiran, lihat Tabel~\ref{tab:example_simulated_dialogue}). Pada tugas **QA terbuka enumeratif**, kami meminta model untuk ``*menghasilkan negara bagian AS*''. Kami pertama-tama mengkueri korpus pra-pelatihan (RedPajama) untuk menetapkan distribusi ``referensi'' nama negara bagian AS dalam data pra-pelatihan. Distribusi probabilitas verbal yang dihasilkan oleh VS, ketika dirata-ratakan selama 10 percobaan, sangat selaras dengan distribusi pra-pelatihan referensi ini (KL=0.12). Sebaliknya, pemicuan langsung runtuh menjadi beberapa mode, berulang kali mengeluarkan negara bagian seperti California dan Texas. Lihat \S\ref{appendix:probing_pre_training_data} untuk detail lebih lanjut.](../figures/intro/qualitative_14.png)
*Contoh kualitatif dan kuantitatif pada tugas yang berbeda. Untuk **penulisan cerita**, VS meningkatkan keragaman keluaran. Untuk tugas **simulasi dialog donasi**, VS mensimulasikan distribusi jumlah donasi yang jauh lebih dekat dengan distribusi manusia, dan menghasilkan perilaku persuasi yang lebih realistis (misalnya, resistensi dan perubahan pikiran, lihat Tabel~\ref{tab:example_simulated_dialogue}). Pada tugas **QA terbuka enumeratif**, kami meminta model untuk ``*menghasilkan negara bagian AS*''. Kami pertama-tama mengkueri korpus pra-pelatihan (RedPajama) untuk menetapkan distribusi ``referensi'' nama negara bagian AS dalam data pra-pelatihan. Distribusi probabilitas verbal yang dihasilkan oleh VS, ketika dirata-ratakan selama 10 percobaan, sangat selaras dengan distribusi pra-pelatihan referensi ini (KL=0.12). Sebaliknya, pemicuan langsung runtuh menjadi beberapa mode, berulang kali mengeluarkan negara bagian seperti California dan Texas. Lihat \S\ref{appendix:probing_pre_training_data} untuk detail lebih lanjut.*
