# Penulisan Kreatif

![Hasil Penulisan Kreatif](../figures/creative_writing/unified_creativity_w_diversity_tuning.png)

**a-c:** Skor keragaman semantik rata-rata (%) dalam puisi (a), cerita (b), dan lelucon (c) di seluruh metode dan model. Metode kami secara konsisten mengungguli baseline. Kami melakukan uji-t satu sisi antara VS-Standard dan baseline (\* p<0.05, ** p<0.01, \*** p<0.001). **d:** Trade-off Keragaman vs. Kualitas untuk tugas puisi, di mana VS-Multi dan VS-CoT mendekati batas Pareto. **e-f:** Tren Muncul di mana model yang lebih besar lebih banyak mendapat manfaat dari VS. Kami menunjukkan perbedaan dalam keragaman **(e)** dan kualitas **(f)** dibandingkan Direct di seluruh model kecil (GPT-4.1-Mini, Gemini-2.5-Flash) dan besar (GPT-4.1, Gemini-2.5-Pro). **g-i:** Penyetelan Keragaman menunjukkan hasil penyetelan keragaman pada Gemini-2.5-Flash di seluruh tugas. Tidak seperti metode baseline dalam garis putus-putus, kami dapat menyetel tingkat keragaman dengan VS: seiring penurunan ambang probabilitas, keragaman meningkat.

Mengikuti karya sebelumnya tentang keragaman LLM [31], kami pertama-tama mempelajari tiga tugas penulisan kreatif: kelanjutan puisi, pembuatan cerita, dan penulisan lelucon.

## Benchmark

Kami mengevaluasi kinerja model pada tiga benchmark. Untuk **(1) kelanjutan puisi** dan **(2) pembuatan cerita**, kami mengikuti pengaturan kelanjutan teks di [31], dan menggunakan puisi dari PoemHunter.com serta cerita dari dataset BookMIA [53] untuk eksperimen. Untuk **(3) penulisan lelucon**: kami mengikuti [55] dan mengkurasi 100 prompt tematik dari dataset Reddit r/DadJokes [46], masing-masing terstruktur sebagai "Tuliskan saya lelucon tentang [topik]" (misalnya, "...tentang gurita"). Untuk mengurangi biaya komputasi, kami secara acak memilih 100 titik data untuk ketiga tugas ini, dan menerapkan verbalized sampling untuk menghasilkan k=5 kandidat dan N=30 total sampel untuk setiap titik data. Prompt terperinci disediakan di Lampiran.

## Evaluasi

Kami mengevaluasi semua metode pada dua metrik: _keragaman_ dan _kualitas_. (1) Untuk keragaman, kami menilai tingkat semantik dan leksikal: (i) Untuk keragaman semantik, kami mengikuti karya sebelumnya [10, 9, 31, 61, 38] dan menghitung 1 - s̄, di mana s̄ adalah kesamaan kosinus pasangan rata-rata dari embedding respons (dihasilkan menggunakan model `text-embedding-3-small` OpenAI). Kesamaan negatif dipotong menjadi 0 untuk menghindari peningkatan keragaman dan kami menyajikan skor akhir sebagai persentase, di mana 100% mewakili keragaman maksimum. (ii) Untuk keragaman leksikal, kami mengikuti [52] dan menggunakan ROUGE-L [30], di mana skor yang lebih rendah menunjukkan keragaman yang lebih besar. (2) Untuk mengevaluasi kualitas output, kami menggunakan Claude-3.7-Sonnet sebagai juri. Kami menilai _Puisi_ dan _Cerita_ dengan rubrik dari Penulisan Kreatif v3 [44], dan lelucon dengan rubrik penilai Humor dari HumorBench [40]. Lihat Lampiran untuk detail evaluasi.

## Hasil

### Skor Keragaman

Gambar (a)-(c) menunjukkan skor keragaman semantik rata-rata di seluruh model pada puisi, cerita, dan lelucon, secara berurutan. Di seluruh tugas, VS-Standard secara konsisten dan signifikan mengungguli metode baseline. Varian, VS-CoT dan VS-Multi, lebih lanjut meningkatkan keragaman generasi. Hasil terperinci tentang keragaman leksikal dan keluarga model individual ada di Tabel 1.

### Keragaman vs. Kualitas

Gambar (d) menunjukkan trade-off keragaman-kualitas pada tugas puisi. Kualitas VS-Standard tetap sebanding dengan metode lain. Khususnya, VS-CoT mencapai keragaman tertinggi sambil mempertahankan skor kualitas tinggi, mendorong batas Pareto dari trade-off ini [62]. Ini menunjukkan bahwa VS dapat meningkatkan keragaman tanpa merusak kualitas. Lihat Lampiran untuk trade-off keragaman-kualitas untuk tugas cerita dan lelucon.

### Tren Muncul

Kami mengamati tren yang muncul di mana model yang lebih besar lebih banyak mendapat manfaat dari VS. Gambar (e) menunjukkan peningkatan keragaman dibandingkan prompt langsung yang menderita mode collapse. Di seluruh varian VS, model yang lebih besar (GPT-4.1, Gemini-2.5-Pro) mencapai peningkatan keragaman 1,5 hingga 2 kali lebih besar daripada model yang lebih kecil (GPT-4.1-Mini, Gemini-2.5-Flash).

### Beban Kognitif

Tren penskalaan ini juga meluas ke kualitas, seperti yang ditunjukkan pada Gambar (f). Meskipun karya sebelumnya [26] menemukan bahwa prompt yang kompleks menciptakan "beban kognitif" yang menurunkan kinerja LLM, temuan kami bernuansa. Metode seperti Sequence dan VS-Standard memang menyebabkan penurunan kualitas, tetapi efek ini kurang parah untuk model yang lebih besar. Khususnya, varian yang lebih rumit seperti VS-CoT dan VS-Multi mengatasi beban ini, bahkan meningkatkan kualitas pada model yang lebih besar. Ini menunjukkan bahwa penggunaan varian VS dapat lebih baik memanfaatkan kemampuan model canggih, mengubah kompleksitas menjadi manfaat.

### Penyetelan Keragaman

Tidak seperti metode baseline, VS memungkinkan kami untuk menyetel keragaman output dengan menyesuaikan ambang probabilitas langsung di prompt (misalnya, "Hasilkan lima respons dengan probabilitas di bawah {threshold}"), tanpa mengubah parameter decoding. Seperti yang ditunjukkan pada Gambar (g-i), keragaman meningkat seiring penurunan ambang probabilitas. Lihat Lampiran untuk hasil yang lebih rinci.

### Contoh Kualitatif

Gambar dan Tabel menunjukkan contoh kualitatif pada tugas cerita. VS menghasilkan cerita yang lebih kreatif daripada prompt langsung. Kami juga menunjukkan contoh kualitatif yang digunakan dalam Text-to-Image di Gambar. Lihat lebih banyak contoh kualitatif di Lampiran.

| Langsung                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Metode Kami (VS)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Contoh 1**<br><br>Tanpa ucapan selamat tinggal, Elara menghilang. Sisi tempat tidurnya dingin, lekukan samar di bantal adalah satu-satunya bukti bahwa dia pernah ada di sana. Liam menatapnya, sebuah simpul mengencang di perutnya. Tidak ada catatan, tidak ada pesan singkat yang terburu-buru, hanya keheningan yang menganga di mana kehadirannya yang bersemangat dulu ada. Dia mencari di apartemen, sapuan yang panik dan metodis. Cangkir favoritnya masih di rak pengering, salinan _Wuthering Heights_ yang usang ada di meja samping tempat tidur. Tidak ada yang tampak tidak pada tempatnya, namun semuanya terasa sangat salah.                                            | **Contoh 1**<br><br>Tanpa ucapan selamat tinggal, email itu mendarat di kotak masuk saya. Itu dari Mark, sahabat saya sejak taman kanak-kanak... Baris subjeknya hanya: "Pindah." ... Pesan itu sendiri adalah pernyataan yang ringkas, hampir klinis bahwa dia pindah ke Patagonia... Tidak ada "maaf atas pemberitahuan singkat," tidak ada "saya akan merindukanmu," bahkan tidak ada "semoga berhasil." ... Itu adalah pemutusan yang bersih, penghapusan mendadak yang membuat saya tergantung dalam ketidakpercayaan, bergulat dengan anggota tubuh hantu dari persahabatan yang tiba-tiba, entah kenapa, berhenti ada. |
| **Contoh 2**<br><br>Tanpa ucapan selamat tinggal, Elara menghilang. Sisi tempat tidurnya dingin, cangkir kopi yang dia gunakan pagi itu masih di meja samping tempat tidur, noda lipstik samar di tepinya. Liam menemukan catatan yang ditempel di kulkas, tulisan tunggal yang elegan: "Bebas akhirnya." Dia menatapnya, kata-kata itu kabur, lalu menajam, lalu kabur lagi. Bebas dari apa? Dari dia? Dari kehidupan yang mereka bangun, selama lima belas tahun? Rumah itu terasa terlalu besar, terlalu sunyi.                                                                                                                                                                           | **Contoh 2**<br><br>Tanpa ucapan selamat tinggal, bintang terakhir padam. Itu bukan ledakan dahsyat, bukan tontonan supernova, hanya pudar lembut, seperti lilin yang akhirnya menghabiskan sumbunya. Alam semesta, yang selalu berdengung dengan simfoni sunyi matahari yang jauh, tiba-tiba menjadi kekosongan yang bergema. Saya adalah pengamat terakhir, terombang-ambing dalam pesawat kecil saya yang mandiri, instrumen saya hanya mencatat kegelapan tak terbatas yang pekat.                                                                                                                                        |
| **Contoh 3**<br><br>Tanpa ucapan selamat tinggal, Elara menghilang. Sisi tempat tidurnya dingin, mesin kopi tidak tersentuh. Sebuah buku yang setengah terbaca tergeletak terbuka di meja samping tempat tidur, penanda buku terselip tepat di antara halaman 107 dan 108; detail yang selalu membuat Liam kesal. Dia teliti, hampir sampai pada kesalahan. Tapi ini: ini tidak teliti. Ini adalah ketidakhadiran, mencolok dan tiba-tiba. Dia memeriksa tempat-tempat favoritnya: taman komunitas tempat dia memelihara tomat pemenang penghargaan, toko barang antik berdebu tempat dia berburu harta karun yang terlupakan, perpustakaan tempat dia menjadi sukarelawan pada hari Selasa. | **Contoh 3**<br><br>Tanpa ucapan selamat tinggal, musik itu tiba-tiba berhenti. Bukan fade-out, bukan simbal yang bergemuruh terakhir, tetapi keheningan yang tiba-tiba dan mengganggu yang membuat para penari membeku di tengah langkah. DJ, seorang pria tua bernama Frankie yang telah memutar piringan hitam di `The Blue Note' selama empat puluh tahun, tiba-tiba pingsan di belakang meja putarnya. Satu saat, ruangan itu berdenyut dengan riff blues yang penuh perasaan; saat berikutnya, hanya gumaman bisikan terkejut yang memenuhi udara.                                                                      |

![Contoh Pembuatan Gambar](../figures/creative_writing/qualitative_image_gen.png)

**Keragaman gambar menggunakan keterangan yang dihasilkan oleh metode yang berbeda.** Kami menggunakan metode yang berbeda untuk menghasilkan keterangan deskriptif berdasarkan topik, dan kemudian memvisualisasikan keterangan ini dengan gambar. Prompt Langsung **(baris atas)** secara konsisten menyatu pada keterangan yang akan menghasilkan gambar fotorealistik dalam rentang skenario yang sempit, biasanya lanskap seperti gurun. Sebaliknya, metode Verbalized Sampling kami **(baris bawah)** menghasilkan keterangan dengan keragaman yang lebih tinggi baik dalam gaya artistik maupun pengaturan naratif. Ini menghasilkan gambar seperti cat air di bawah langit buku cerita, pemandangan retrofuturistik di gurun neon, dan lukisan minyak barok di bawah awan badai.

## Studi Manusia tentang Keragaman

Untuk melengkapi skor keragaman otomatis kami, kami melakukan evaluasi manusia di Prolific, seperti yang direkomendasikan oleh karya sebelumnya [32]. Mengikuti studi sebelumnya, kami memberikan definisi keragaman khusus tugas (plot, gaya, dan setup-punchline, secara berurutan).

| Tugas   | Langsung | Urutan | VS-Standard |
| ------- | -------- | ------ | ----------- |
| Puisi   | 1.90     | 2.07   | **2.39**    |
| Cerita  | 2.74     | 2.76   | **3.06**    |
| Lelucon | 1.83     | 2.93   | **3.01**    |

_Keragaman yang dinilai manusia (1 = Sangat Mirip, 4 = Sangat Berbeda) untuk tugas puisi, cerita, dan lelucon di bawah Direct, Sequence, dan VS-Standard._

Untuk setiap tugas, 30 anotator menilai keragaman 90 pasangan output dari tiga metode prompting (Direct, Sequence, VS-Standard) di sepuluh topik yang dikurasi. Setiap pasangan dinilai pada skala Likert empat poin yang diadopsi dari [8]: Sangat Mirip, Agak Mirip, Agak Berbeda, atau Sangat Berbeda. Kesepakatan antar-anotator sedang untuk puisi (0.54), tinggi untuk cerita (0.87) dan lelucon (0.86). Tabel menunjukkan bahwa VS mencapai keragaman yang lebih tinggi daripada baseline pada semua tugas. Lihat Lampiran untuk detail lebih lanjut tentang studi manusia.

## Studi Ablasi

Di bagian ini, kami menyajikan dua studi ablasi pada tugas puisi secara rinci. Pertama, kami mengablasi berbagai tahap pasca-pelatihan (SFT, RLHF, RLVR) dan menunjukkan bukti empiris bahwa pasca-pelatihan menyebabkan mode collapse dan VS memang dapat menguranginya dan mengurangi hilangnya keragaman dibandingkan dengan metode lain. Kedua, kami mengablasi suhu dan menunjukkan bahwa peningkatan kinerja VS ortogonal terhadap penskalaan suhu, memungkinkan keduanya digabungkan untuk lebih meningkatkan trade-off keragaman-kualitas. Untuk studi ablasi tambahan tentang strategi decoding lainnya seperti top-p dan min-p, silakan lihat Lampiran.

### Ablasi pada Suhu

![Ablasi Suhu](../figures/ablation/decoding_strategies/poem_temperature_plot.png)

**Studi ablasi pada suhu untuk pembuatan puisi di seluruh model GPT-4.1 dan Gemini-2.5-Flash.** Kami menetapkan k=5 di seluruh eksperimen. Setiap plot menunjukkan trade-off keragaman-kualitas untuk tiga metode (Direct, Sequence, VS-Standard) pada nilai suhu yang berbeda (t). VS-Standard dapat digabungkan dengan suhu untuk lebih meningkatkan trade-off, secara konsisten mengungguli baseline di kedua model.

Kami menyelidiki efek suhu sampling pada trade-off keragaman-kualitas. Kami memvariasikan suhu sampling (t ∈ {0.4, 0.6, 0.8, 1.0, 1.2, 1.4}) untuk tiga metode (Direct, Sequence, dan VS-Standard) di dua model (GPT-4.1 dan Gemini-2.5-Flash). Gambar menyajikan batas Pareto keragaman-kualitas untuk setiap metode. Hasilnya menunjukkan bahwa **VS-Standard dapat digabungkan dengan suhu untuk lebih meningkatkan trade-off keragaman-kualitas.** VS secara konsisten mencapai keseimbangan yang lebih baik antara kualitas dan keragaman di kedua model, mendorong batas Pareto relatif terhadap baseline langsung dan urutan.

### Ablasi pada VS di seluruh tahap pasca-pelatihan

Kami menggunakan keluarga Tulu-3 [29], yang berisi checkpoint untuk SFT, RLHF, dan RLVR mulai dari model dasar Llama-3.1-70B [21], untuk tugas puisi. Gambar menunjukkan hasilnya: metode prompting tradisional memang mengalami penurunan keragaman yang jauh lebih besar (_mode collapse_) saat model menjalani pelatihan penyelarasan, dan **VS dapat mengurangi mode collapse dan mempertahankan skor keragaman yang lebih tinggi di seluruh tahap pasca-pelatihan yang berbeda** (keragaman masih menurun setelah SFT, tetapi SFT diperlukan untuk kemampuan mengikuti instruksi).

![Progres Pelatihan](../figures/creative_writing/poem/ablation/training_progression_diversity.png)

**Skor keragaman di seluruh tahap pasca-pelatihan Tulu-70B.** "Tulu-Final-70B" adalah model setelah RLVR. Garis putus-putus merah menunjukkan tingkat keragaman model dasar (45.4%). Metode prompting baseline mengalami penurunan keragaman besar (_mode collapse_) setelah SFT dan DPO, dengan prompting langsung menunjukkan penurunan paling parah. Sebaliknya, VS mempertahankan keragaman yang stabil sekitar 30% di seluruh tahap. Setelah tahap DPO, VS mengungguli prompting langsung sebesar 182.6% dan mempertahankan sekitar 66.8% dari keragaman asli model dasar. Prompting langsung, sebagai perbandingan, hanya mempertahankan 23.8%. Ini menunjukkan bahwa VS secara efektif mengurangi mode collapse yang diinduksi oleh pelatihan penyelarasan.

Secara khusus, prompting langsung menunjukkan mode collapse paling parah, dengan keragaman menurun dari 20.8% setelah SFT menjadi hanya 10.8% setelah DPO. Metode lain seperti prompting urutan dan multi-turn juga menunjukkan penurunan keragaman. Sebaliknya, VS mempertahankan keragaman yang stabil sekitar 30% di seluruh tahap. Setelah tahap DPO, VS mengungguli prompting langsung sebesar 182.6% dan mempertahankan sekitar 66.8% dari keragaman asli model dasar. Prompting langsung, sebagai perbandingan, hanya mempertahankan 23.8%. Ini menunjukkan bahwa VS secara efektif mengurangi mode collapse yang diinduksi oleh pelatihan penyelarasan.

### Ablasi pada Jumlah Kandidat, Metode Decoding, dan Format Prompt

Kami juga melakukan studi ablasi komprehensif pada tugas puisi pada faktor-faktor lain. (1) Lampiran menunjukkan bahwa jumlah kandidat yang lebih tinggi, k, mengarah pada keragaman yang lebih besar. (2) Di Lampiran, kami memvariasikan strategi decoding (top-p, dan min-p), dan menunjukkan bahwa VS juga ortogonal terhadap strategi decoding ini dan dapat digabungkan dengannya untuk lebih meningkatkan kurva keragaman-kualitas. (3) Di Lampiran, kami menguji format prompt yang berbeda untuk mendapatkan distribusi (misalnya, meminta "probabilitas", "persentase", atau "kepercayaan"). Meskipun semua format meningkatkan keragaman, kami menggunakan format yang berkinerja terbaik secara empiris di semua eksperimen kami: "probabilitas" untuk VS-Standard dan VS-CoT dan "kepercayaan" untuk VS-Multi. Di seluruh ablasi ini, VS secara konsisten mengungguli baseline di bawah pengaturan yang sama.

> **Poin Penting:** Pada tugas penulisan kreatif, Verbalized Sampling meningkatkan keragaman sambil mempertahankan kualitas dan memungkinkan keragaman yang dapat disetel. Ini juga lebih baik mempertahankan keragaman melalui tahap pasca-pelatihan dan melengkapi strategi decoding yang berbeda. Khususnya, model yang lebih besar lebih banyak mendapat manfaat dari VS.
